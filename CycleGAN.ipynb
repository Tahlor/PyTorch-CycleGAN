{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import utils\n",
    "import models\n",
    "import datasets\n",
    "\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from models import Generator\n",
    "from models import Discriminator\n",
    "from utils import ReplayBuffer\n",
    "from utils import LambdaLR\n",
    "from utils import Logger\n",
    "from utils import weights_init_normal\n",
    "from datasets import ImageDataset\n",
    "\n",
    "import hwr\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--offline_root'], dest='offline_root', nargs=None, const=None, default='datasets/horse2zebra/trainB', type=<class 'str'>, choices=None, help='root directory of the dataset', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shlex\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epoch', type=int, default=0, help='starting epoch')\n",
    "parser.add_argument('--n_epochs', type=int, default=200, help='number of epochs of training')\n",
    "parser.add_argument('--batchSize', type=int, default=1, help='size of the batches')\n",
    "#parser.add_argument('--dataroot', type=str, default='datasets/horse2zebra/', help='root directory of the dataset')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate')\n",
    "parser.add_argument('--decay_epoch', type=int, default=100, help='epoch to start linearly decaying the learning rate to 0')\n",
    "parser.add_argument('--sizeX', type=int, default=256, help='size of the data crop (squared assumed)')\n",
    "parser.add_argument('--sizeY', type=int, default=256, help='size of the data crop (squared assumed)')\n",
    "parser.add_argument('--input_nc', type=int, default=3, help='number of channels of input data')\n",
    "parser.add_argument('--output_nc', type=int, default=3, help='number of channels of output data')\n",
    "parser.add_argument('--cuda', action='store_true', help='use GPU computation')\n",
    "parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
    "parser.add_argument('--online_root', type=str, default='datasets/horse2zebra/trainA', help='root directory of the dataset')\n",
    "parser.add_argument('--offline_root', type=str, default='datasets/horse2zebra/trainB', help='root directory of the dataset')\n",
    "\n",
    "\n",
    "#opt = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating newtorks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/site-packages/torch/cuda/__init__.py:118: UserWarning: \n",
      "    Found GPU1 GeForce GTX 660 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n",
      "/media/data/GitHub/PyTorch-CycleGAN/utils.py:121: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating optimizers...\n",
      "Creating inputs...\n",
      "Creating dataloaders....\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    cycle_loss = True\n",
    "    identity_loss = True\n",
    "    B2A = True\n",
    "    batch = 2\n",
    "else:\n",
    "    cycle_loss = False\n",
    "    identity_loss = False\n",
    "    B2A = False\n",
    "    batch = 12\n",
    "    \n",
    "args = f\"--epoch 0 --cuda --input_nc 1 --output_nc 1 --batchSize {batch} --sizeX 64 --sizeY 1280 --online_root /media/data/GitHub/handwriting_data/train_online_cropped --offline_root /media/data/GitHub/handwriting_data/train_offline_preprocessed \"\n",
    "opt = parser.parse_args(shlex.split(args))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = \"cuda\" if opt.cuda else \"cpu\"\n",
    "###### Definition of variables ######\n",
    "# Networks\n",
    "print(\"Creating newtorks\")\n",
    "\n",
    "netG_A2B = Generator(opt.input_nc, opt.output_nc)\n",
    "netG_B2A = Generator(opt.output_nc, opt.input_nc)\n",
    "netD_A = Discriminator(opt.input_nc)\n",
    "netD_B = Discriminator(opt.output_nc)\n",
    "\n",
    "if opt.cuda:\n",
    "    netG_A2B.cuda()\n",
    "    netG_B2A.cuda()\n",
    "    netD_A.cuda()\n",
    "    netD_B.cuda()\n",
    "\n",
    "netG_A2B.apply(weights_init_normal)\n",
    "netG_B2A.apply(weights_init_normal)\n",
    "netD_A.apply(weights_init_normal)\n",
    "netD_B.apply(weights_init_normal)\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "print(\"Creating optimizers...\")\n",
    "# Optimizers & LR schedulers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "                                lr=opt.lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
    "\n",
    "print(\"Creating inputs...\")\n",
    "# Inputs & targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor\n",
    "input_A = Tensor(opt.batchSize, opt.input_nc, opt.sizeX, opt.sizeY)\n",
    "input_B = Tensor(opt.batchSize, opt.output_nc, opt.sizeX, opt.sizeY)\n",
    "target_real = Variable(Tensor(opt.batchSize).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(opt.batchSize).fill_(0.0), requires_grad=False)\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "print(\"Creating dataloaders....\")\n",
    "# Dataset loader\n",
    "#                 transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) \n",
    "transforms_ = [ transforms.ToTensor()]\n",
    "dataloader = DataLoader(ImageDataset(opt.online_root, opt.offline_root, transforms_=transforms_, unaligned=True), \n",
    "                        batch_size=opt.batchSize, shuffle=True, num_workers=opt.n_cpu)\n",
    "\n",
    "# Loss plot\n",
    "logger = Logger(opt.n_epochs, len(dataloader))\n",
    "###################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensors(tensor):\n",
    "    for i in range(0, tensor.shape[0]):\n",
    "        plot_tensor(tensor[i,0])\n",
    "\n",
    "def plot_tensor(tensor):\n",
    "    print(tensor.shape)\n",
    "    t = tensor.cpu()\n",
    "    assert not np.isnan(t).any()\n",
    "    plt.figure(dpi=400)\n",
    "    plt.imshow(t, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'crnn.CNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'crnn.BidirectionalRNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting first epoch\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/site-packages/torch/nn/modules/loss.py:443: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/200 [0286/6598] -- loss_G: 2.4147 | loss_G_identity: 0.5284 | loss_G_GAN: 0.7876 | loss_G_cycle: 1.0987 | loss_D: 0.5021 | hw_loss: 0.0000 -- ETA: 10 days, 9:28:45.433765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/taylor/anaconda3/envs/cyclegan/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-907e700a256a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mloss_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_identity_A\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_identity_B\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_GAN_A2B\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_GAN_B2A\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_cycle_ABA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_cycle_BAB\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhw_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mloss_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cyclegan/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cyclegan/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_hw_loss = False\n",
    "hw = hwr.HWR(batch_size=opt.batchSize)\n",
    "zero_tensor = torch.tensor([0.0], requires_grad=False).to(device)\n",
    "\n",
    "###### Training ######\n",
    "print(\"Starting first epoch\")\n",
    "for epoch in range(opt.epoch, opt.n_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Set model input\n",
    "        real_A = Variable(input_A.copy_(batch['A']))\n",
    "        real_B = Variable(input_B.copy_(batch['B']))\n",
    "               \n",
    "        # Label\n",
    "        if use_hw_loss:\n",
    "            t = hw.hwr_predict(real_A[:,:,2:62,:], as_string=False)\n",
    "            label_A, label_A_lengths = hwr.build_labels(t, device)\n",
    "\n",
    "            # Ensure HWR is working:\n",
    "            if False:\n",
    "                plot_tensors(real_A)\n",
    "                string_prediction = hw.hwr_predict(real_A[:,:,2:62,:])\n",
    "                print(string_prediction)\n",
    "                input()\n",
    "        \n",
    "        ###### Generators A2B and B2A ######\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        if identity_loss:\n",
    "            # G_A2B(B) should equal B if real B is fed\n",
    "            same_B = netG_A2B(real_B)\n",
    "            loss_identity_B = criterion_identity(same_B, real_B)*5.0\n",
    "            # G_B2A(A) should equal A if real A is fed\n",
    "            same_A = netG_B2A(real_A)\n",
    "            loss_identity_A = criterion_identity(same_A, real_A)*5.0\n",
    "        else:\n",
    "            loss_identity_A = zero_tensor\n",
    "            loss_identity_B = zero_tensor\n",
    "            \n",
    "        # GAN loss\n",
    "        fake_B = netG_A2B(real_A)\n",
    "        pred_fake = netD_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, target_real)\n",
    "\n",
    "        if B2A:\n",
    "            fake_A = netG_B2A(real_B)\n",
    "            pred_fake = netD_A(fake_A)\n",
    "            loss_GAN_B2A = criterion_GAN(pred_fake, target_real)\n",
    "        else:\n",
    "            loss_GAN_B2A = zero_tensor\n",
    "            fake_A = zero_tensor\n",
    "            \n",
    "            \n",
    "        # Cycle loss\n",
    "        if cycle_loss:\n",
    "            recovered_A = netG_B2A(fake_B)\n",
    "            loss_cycle_ABA = criterion_cycle(recovered_A, real_A)*10.0\n",
    "\n",
    "            recovered_B = netG_A2B(fake_A)\n",
    "            loss_cycle_BAB = criterion_cycle(recovered_B, real_B)*10.0\n",
    "        else:\n",
    "            loss_cycle_BAB = zero_tensor\n",
    "            loss_cycle_ABA = zero_tensor\n",
    "        \n",
    "#         # Handwriting loss\n",
    "#         hw_loss = hw.hwr_loss(real_A[:,:,2:62,:], label_A, label_A_lengths)\n",
    "#         string_prediction = hw.hwr_predict(real_A[:,:,2:62,:], as_string=False)\n",
    "#         print(f\"Real Prediction {string_prediction}\")\n",
    "#         print(f\"HW loss REAL: {hw_loss}\")\n",
    "\n",
    "#         string_prediction = hw.hwr_predict(fake_B[:,:,2:62,:], as_string=False)\n",
    "#         print(f\"Fake Prediction {string_prediction}\")\n",
    "#         hw_loss = hw.hwr_loss(fake_B[:,:,2:62,:], label_A, label_A_lengths)\n",
    "#         print(f\"HW loss FAKE: {hw_loss}\")\n",
    "#         print(label_A, label_A_lengths)\n",
    "#         Stop\n",
    "        \n",
    "        if use_hw_loss:\n",
    "            hw_loss = hw.hwr_loss(fake_B[:,:,2:62,:], label_A, label_A_lengths) / 10\n",
    "            if torch.isinf(hw_loss):\n",
    "                warnings.warn(f\"Encountered {hw_loss} hw_loss\")\n",
    "                hw_loss = zero_tensor\n",
    "        else:\n",
    "            hw_loss = zero_tensor\n",
    "        # np.nanmax(np.isfinite([hw_loss,0]))\n",
    "        \n",
    "        # Total loss\n",
    "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB + hw_loss\n",
    "        loss_G.backward()\n",
    "        \n",
    "        optimizer_G.step()\n",
    "        \n",
    "        if np.isnan(loss_G.item()):\n",
    "            for i in range(0,real_A.shape[0]):\n",
    "                plot_tensor(real_A[i,0])\n",
    "                plot_tensor(real_B[i,0])\n",
    "                \n",
    "            Stop\n",
    "        \n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator A ######\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = netD_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        if B2A:\n",
    "            fake_A = fake_A_buffer.push_and_pop(fake_A)\n",
    "            pred_fake = netD_A(fake_A.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "        else:\n",
    "            loss_D_fake = zero_tensor\n",
    "            \n",
    "        # Total loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_A.backward()\n",
    "\n",
    "        optimizer_D_A.step()\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator B ######\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = netD_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "        \n",
    "        # Fake loss\n",
    "        fake_B = fake_B_buffer.push_and_pop(fake_B)\n",
    "        pred_fake = netD_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_B.backward()\n",
    "\n",
    "        optimizer_D_B.step()\n",
    "        ###################################\n",
    "\n",
    "        # Progress report (http://localhost:8097)\n",
    "        logger.log({'loss_G': loss_G, 'loss_G_identity': (loss_identity_A + loss_identity_B), 'loss_G_GAN': (loss_GAN_A2B + loss_GAN_B2A),\n",
    "                    'loss_G_cycle': (loss_cycle_ABA + loss_cycle_BAB), 'loss_D': (loss_D_A + loss_D_B), 'hw_loss':hw_loss}, \n",
    "                    images={'real_A': real_A, 'real_B': real_B, 'fake_A': fake_A, 'fake_B': fake_B})\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    # Save models checkpoints\n",
    "    torch.save(netG_A2B.state_dict(), 'output/netG_A2B.pth')\n",
    "    torch.save(netG_B2A.state_dict(), 'output/netG_B2A.pth')\n",
    "    torch.save(netD_A.state_dict(), 'output/netD_A.pth')\n",
    "    torch.save(netD_B.state_dict(), 'output/netD_B.pth')\n",
    "###################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([[0],[0]]).repeat(1,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_size = Variable(torch.IntTensor([x.size(0)] * x.size(1)))\n",
    "print(preds_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc = torch.nn.CTCLoss()\n",
    "log_softmax = torch.nn.LogSoftmax(dim=2).to(device)\n",
    "ctc_criterion = lambda x, y, z, t: ctc(log_softmax(x), y, z, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t)\n",
    "tt = torch.tensor(t).to(device)\n",
    "inputs_size = Variable(torch.IntTensor([t.size(0)] * t.size(1)))\n",
    "ctc_criterion(t, label_A, inputs_size, label_A_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_tensor = torch.tensor([[0.0]], requires_grad=False).to(device)\n",
    "print(zero_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclegan",
   "language": "python",
   "name": "cyclegan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
